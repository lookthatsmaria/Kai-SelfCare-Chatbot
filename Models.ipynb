{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading The Dataset \n",
    "df = pd.read_csv('Dataset_Cognitive_Distortions.tsv', sep='\\t', header=0)\n",
    "#df = df.drop(df.columns[[2, 3, 4]], axis=1)  # df.columns is zero-based pd.Index\n",
    "df.shape\n",
    "\n",
    "#Data Preprocessing \n",
    "df['Phrase'] = df['Phrase'].str.lower()\n",
    "\n",
    "\n",
    "# Divide data between training and test data\n",
    "X = df['Phrase']\n",
    "y = df['Cognitive Distortion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n",
      "Train Accuracy : 0.994\n",
      "Test Accuracy : 0.689\n",
      "Best Accuracy Through Grid Search : 0.683\n",
      "Best Parameters :  {'mnb__alpha': 0.1, 'smote__k_neighbors': 10}\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  5  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  1  0  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1  0  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  2  0  0  4  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  1  4  1  1  2  0]\n",
      " [ 0  1  0  1  0  1  0  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  0  0  0  1  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      1.00      1.00         3\n",
      "                Blaming       0.62      0.71      0.67         7\n",
      "        Catastrophizing       0.83      0.83      0.83         6\n",
      "      Control Fallacies       0.64      0.54      0.58        13\n",
      "    Emotional Reasoning       1.00      1.00      1.00        11\n",
      "      Fallacy Of Change       0.67      1.00      0.80         2\n",
      "    Fallacy Of Fairness       1.00      0.50      0.67         6\n",
      "              Filtering       0.50      0.60      0.55        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.56      0.83      0.67         6\n",
      " Jumping to Conclusions       0.80      0.44      0.57         9\n",
      "                     No       0.44      0.36      0.40        11\n",
      "     Overgeneralization       0.71      0.62      0.67         8\n",
      "        Personalization       0.71      1.00      0.83         5\n",
      "              Polarized       0.33      0.50      0.40         6\n",
      "                Shoulds       1.00      0.71      0.83         7\n",
      "\n",
      "               accuracy                           0.69       119\n",
      "              macro avg       0.73      0.73      0.71       119\n",
      "           weighted avg       0.71      0.69      0.69       119\n",
      "\n",
      "0.6890756302521008\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('mnb', MultinomialNB())\n",
    "])\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ],\n",
    "          'mnb__alpha': [0.01, 0.1, 0.3, 0.5, 1.0, 10, ]}\n",
    "          \n",
    "#textclassifier.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTINOMIAL LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "2700 fits failed out of a total of 7200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.07562057 0.0777039  0.0777039\n",
      " 0.0777039  0.0777039  0.0777039  0.0777039  0.0777039  0.0777039\n",
      " 0.56724291 0.55882092 0.56733156 0.57353723 0.57384752 0.58191489\n",
      " 0.58009752 0.57367021 0.59060284 0.56724291 0.55882092 0.56733156\n",
      " 0.57353723 0.57384752 0.58191489 0.58009752 0.57367021 0.59060284\n",
      " 0.56099291 0.55673759 0.56737589 0.57140957 0.57176418 0.58191489\n",
      " 0.58847518 0.56733156 0.59268617 0.52544326 0.51901596 0.51910461\n",
      " 0.53169326 0.51919326 0.52109929 0.52969858 0.52522163 0.5402039\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.07367021 0.06941489 0.07575355\n",
      " 0.07579787 0.08195922 0.07575355 0.07371454 0.08413121 0.06941489\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.05465426 0.0545656  0.06103723\n",
      " 0.05678191 0.0545656  0.05886525 0.0483156  0.05682624 0.05669326\n",
      " 0.57570922 0.56932624 0.57783688 0.58404255 0.58435284 0.59033688\n",
      " 0.58847518 0.58413121 0.60115248 0.57570922 0.56932624 0.57783688\n",
      " 0.58404255 0.58435284 0.59033688 0.58847518 0.58413121 0.60115248\n",
      " 0.57570922 0.56932624 0.57783688 0.58195922 0.5822695  0.59033688\n",
      " 0.58847518 0.58413121 0.60115248 0.57575355 0.56307624 0.57570922\n",
      " 0.57566489 0.57805851 0.58191489 0.57792553 0.58204787 0.5947695\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.09862589 0.10088652 0.09237589\n",
      " 0.1008422  0.09875887 0.1008422  0.09875887 0.09875887 0.1008422\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.16369681 0.15527482 0.16374113\n",
      " 0.16578014 0.16369681 0.1554078  0.13860816 0.14911348 0.16573582\n",
      " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
      " 0.58838652 0.5945922  0.61578014 0.59680851 0.59024823 0.58408688\n",
      " 0.58617021 0.59689716 0.60079787 0.58838652 0.5945922  0.61578014\n",
      " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
      " 0.58838652 0.5945922  0.61578014 0.59468085 0.58603723 0.58200355\n",
      " 0.58617021 0.59902482 0.60496454 0.59671986 0.59667553 0.6179078\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.3106383  0.32544326 0.27096631\n",
      " 0.29401596 0.31697695 0.3043883  0.2918883  0.29809397 0.30434397\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.32101064 0.32960993 0.28125\n",
      " 0.30873227 0.28351064 0.29175532 0.28156028 0.29175532 0.28320035\n",
      " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
      " 0.60305851 0.60305851 0.63049645 0.60296986 0.60279255 0.60718085\n",
      " 0.60088652 0.60939716 0.60921986 0.60305851 0.60305851 0.63049645\n",
      " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
      " 0.60305851 0.60305851 0.63049645 0.60301418 0.59862589 0.60509752\n",
      " 0.60500887 0.6114805  0.61130319 0.6072695  0.60509752 0.63049645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.41143617 0.40935284 0.38661348\n",
      " 0.39268617 0.39507979 0.40323582 0.38444149 0.38639184 0.39073582\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.45602837 0.4623227  0.44135638\n",
      " 0.43275709 0.43289007 0.43927305 0.44140071 0.45164007 0.42646277\n",
      " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
      " 0.64069149 0.65124113 0.66609043 0.64490248 0.64277482 0.64924645\n",
      " 0.64281915 0.63674645 0.63005319 0.64069149 0.65124113 0.66609043\n",
      " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
      " 0.64069149 0.65124113 0.66609043 0.64703014 0.64069149 0.64924645\n",
      " 0.64281915 0.63253546 0.62796986 0.63860816 0.64911348 0.66187943\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.51050532 0.50629433 0.49804965\n",
      " 0.48949468 0.49818262 0.50629433 0.50234929 0.4981383  0.50647163\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.647\n",
      "Best Accuracy Through Grid Search : 0.687\n",
      "Best Parameters :  {'mlg__C': 0.01, 'mlg__penalty': 'none', 'mlg__solver': 'saga', 'smote__k_neighbors': 2}\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  2  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  1  0  4  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0  0  0  5  0  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  1  1  4  1  1  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  1  1  0  0  0  1  0  0  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       0.75      1.00      0.86         3\n",
      "                Blaming       0.60      0.43      0.50         7\n",
      "        Catastrophizing       0.71      0.83      0.77         6\n",
      "      Control Fallacies       0.70      0.54      0.61        13\n",
      "    Emotional Reasoning       0.92      1.00      0.96        11\n",
      "      Fallacy Of Change       1.00      1.00      1.00         2\n",
      "    Fallacy Of Fairness       0.25      0.17      0.20         6\n",
      "              Filtering       0.50      0.40      0.44        10\n",
      "       Global Labelling       1.00      1.00      1.00         9\n",
      "Heaven's Reward Fallacy       0.45      0.83      0.59         6\n",
      " Jumping to Conclusions       0.71      0.56      0.63         9\n",
      "                     No       0.36      0.36      0.36        11\n",
      "     Overgeneralization       0.60      0.75      0.67         8\n",
      "        Personalization       0.71      1.00      0.83         5\n",
      "              Polarized       0.29      0.33      0.31         6\n",
      "                Shoulds       1.00      0.71      0.83         7\n",
      "\n",
      "               accuracy                           0.65       119\n",
      "              macro avg       0.66      0.68      0.66       119\n",
      "           weighted avg       0.65      0.65      0.64       119\n",
      "\n",
      "0.6470588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('mlg', LogisticRegression(multi_class='multinomial', random_state=0, warm_start = True,  l1_ratio = 0.5))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ],\n",
    "          'mlg__penalty': ['l1', 'l2', 'elasticnet', 'none' ],\n",
    "          'mlg__C': [0.01, 0.1, 0.3, 0.5, 1.0,],\n",
    "          'mlg__solver': ['lbfgs', 'newton-cg'\n",
    "                          , 'sag', 'saga'],\n",
    "          }\n",
    "          \n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 22 candidates, totalling 220 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 220.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "6 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\base.py\", line 203, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\base.py\", line 88, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 355, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 810, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 16, n_neighbors = 17\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "14 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 293, in fit\n",
      "    Xt, yt = self._fit(X, y, **fit_params_steps)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 250, in _fit\n",
      "    X, y, fitted_transformer = fit_resample_one_cached(\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\joblib\\memory.py\", line 349, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 422, in _fit_resample_one\n",
      "    X_res, y_res = sampler.fit_resample(X, y, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\base.py\", line 203, in fit_resample\n",
      "    return super().fit_resample(X, y)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\base.py\", line 88, in fit_resample\n",
      "    output = self._fit_resample(X, y)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\over_sampling\\_smote\\base.py\", line 355, in _fit_resample\n",
      "    nns = self.nn_k_.kneighbors(X_class, return_distance=False)[:, 1:]\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 810, in kneighbors\n",
      "    raise ValueError(\n",
      "ValueError: Expected n_neighbors <= n_samples,  but n_samples = 15, n_neighbors = 17\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.62836879 0.62836879 0.62836879 0.62836879 0.63031915 0.63031915\n",
      " 0.62615248 0.62615248 0.63244681 0.63244681 0.62615248 0.62615248\n",
      " 0.63031915 0.63031915 0.62828014 0.62828014 0.63670213 0.63670213\n",
      " 0.63453014 0.63453014        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.672\n",
      "Best Accuracy Through Grid Search : 0.637\n",
      "Best Parameters :  {'smote__k_neighbors': 10, 'svm__decision_function_shape': 'ovo'}\n",
      "[[ 2  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  3  0  1  0  0  0  0  0  0  0  2  0  1  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  0  2  1  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  5  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  6  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  2  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  4  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  3  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      0.67      0.80         3\n",
      "                Blaming       1.00      0.43      0.60         7\n",
      "        Catastrophizing       1.00      0.67      0.80         6\n",
      "      Control Fallacies       0.78      0.54      0.64        13\n",
      "    Emotional Reasoning       1.00      1.00      1.00        11\n",
      "      Fallacy Of Change       1.00      0.50      0.67         2\n",
      "    Fallacy Of Fairness       0.00      0.00      0.00         6\n",
      "              Filtering       0.67      0.40      0.50        10\n",
      "       Global Labelling       1.00      1.00      1.00         9\n",
      "Heaven's Reward Fallacy       0.83      0.83      0.83         6\n",
      " Jumping to Conclusions       0.86      0.67      0.75         9\n",
      "                     No       0.31      1.00      0.47        11\n",
      "     Overgeneralization       0.75      0.75      0.75         8\n",
      "        Personalization       0.80      0.80      0.80         5\n",
      "              Polarized       0.33      0.33      0.33         6\n",
      "                Shoulds       0.83      0.71      0.77         7\n",
      "\n",
      "               accuracy                           0.67       119\n",
      "              macro avg       0.76      0.64      0.67       119\n",
      "           weighted avg       0.75      0.67      0.67       119\n",
      "\n",
      "0.6722689075630253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('svm', svm.SVC(probability = True))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10,],\n",
    "        'svm__decision_function_shape': ['ovo', 'ovr']}\n",
    "          \n",
    "#textclassifier.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.689\n",
      "Best Accuracy Through Grid Search : 0.668\n",
      "Best Parameters :  {'smote__k_neighbors': 3}\n",
      "[[2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 4 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 2 0 6 1 0 0 0 0 0 0 2 0 0 0 2]\n",
      " [0 0 0 0 9 0 0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 5 0 0 0 3 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 2 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 7 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 9 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 5 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 1 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      0.67      0.80         3\n",
      "                Blaming       0.62      0.71      0.67         7\n",
      "        Catastrophizing       0.80      0.67      0.73         6\n",
      "      Control Fallacies       0.75      0.46      0.57        13\n",
      "    Emotional Reasoning       0.82      0.82      0.82        11\n",
      "      Fallacy Of Change       1.00      1.00      1.00         2\n",
      "    Fallacy Of Fairness       0.50      0.17      0.25         6\n",
      "              Filtering       0.56      0.50      0.53        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.75      0.50      0.60         6\n",
      " Jumping to Conclusions       1.00      0.78      0.88         9\n",
      "                     No       0.53      0.82      0.64        11\n",
      "     Overgeneralization       0.56      0.62      0.59         8\n",
      "        Personalization       0.83      1.00      0.91         5\n",
      "              Polarized       0.60      0.50      0.55         6\n",
      "                Shoulds       0.54      1.00      0.70         7\n",
      "\n",
      "               accuracy                           0.69       119\n",
      "              macro avg       0.73      0.70      0.70       119\n",
      "           weighted avg       0.71      0.69      0.68       119\n",
      "\n",
      "0.6890756302521008\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('rf', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ]}\n",
    "          \n",
    "#textclassifier.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Train Accuracy : 0.996\n",
      "Test Accuracy : 0.605\n",
      "Best Accuracy Through Grid Search : 0.595\n",
      "Best Parameters :  {'smote__k_neighbors': 10}\n",
      "[[3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 0 0 0 0 0 0 0 0 0 4 0 0 0]\n",
      " [0 0 4 0 0 0 0 0 0 1 0 0 0 0 1 0]\n",
      " [0 0 0 5 2 0 0 0 0 0 0 3 1 0 0 2]\n",
      " [0 0 0 0 6 0 0 0 0 0 0 0 3 0 0 2]\n",
      " [0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 1 0 0 0 0 1 1 0 0 2]\n",
      " [0 1 0 1 0 1 0 4 0 1 0 1 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 8 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 4 0 0 0 0 0 0]\n",
      " [0 0 0 1 1 0 0 0 0 0 7 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 2 7 1 0 0 0]\n",
      " [0 0 0 0 0 1 0 1 0 0 0 0 5 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 1 1 0 0 0 0 1 0 1 0 0 0 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      1.00      1.00         3\n",
      "                Blaming       0.75      0.43      0.55         7\n",
      "        Catastrophizing       0.67      0.67      0.67         6\n",
      "      Control Fallacies       0.56      0.38      0.45        13\n",
      "    Emotional Reasoning       0.67      0.55      0.60        11\n",
      "      Fallacy Of Change       0.33      0.50      0.40         2\n",
      "    Fallacy Of Fairness       1.00      0.17      0.29         6\n",
      "              Filtering       0.57      0.40      0.47        10\n",
      "       Global Labelling       0.89      0.89      0.89         9\n",
      "Heaven's Reward Fallacy       0.67      0.67      0.67         6\n",
      " Jumping to Conclusions       0.58      0.78      0.67         9\n",
      "                     No       0.58      0.64      0.61        11\n",
      "     Overgeneralization       0.31      0.62      0.42         8\n",
      "        Personalization       1.00      1.00      1.00         5\n",
      "              Polarized       0.67      0.33      0.44         6\n",
      "                Shoulds       0.50      1.00      0.67         7\n",
      "\n",
      "               accuracy                           0.61       119\n",
      "              macro avg       0.67      0.63      0.61       119\n",
      "           weighted avg       0.65      0.61      0.60       119\n",
      "\n",
      "0.6050420168067226\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('lgbm', LGBMClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ]}\n",
    "          \n",
    "#textclassifier.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 108 candidates, totalling 1080 fits\n",
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.529\n",
      "Best Accuracy Through Grid Search : 0.540\n",
      "Best Parameters :  {'knn__n_neighbors': 5, 'knn__weights': 'distance', 'smote__k_neighbors': 10}\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {'knn__n_neighbors': [5,6,7,8,9,10, ],\n",
    "          'knn__weights': ['uniform', 'distance'],\n",
    "          'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ]\n",
    "          }\n",
    "          \n",
    "#textclassifier.fit(X_train, y_train)\n",
    "\n",
    "#y_pred = textclassifier.predict(X_test)\n",
    "\n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 1 0 0 2 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 2 2 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 3 0 7 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [2 0 0 0 7 0 0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 3 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 7 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 4 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0 3 0 0 1 3 0]\n",
      " [1 0 0 0 0 1 1 3 0 1 0 0 0 2 2 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 4 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 1 0 1 0 0 1 0 0 0 0 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 2 3]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       0.33      1.00      0.50         3\n",
      "                Blaming       0.50      0.43      0.46         7\n",
      "        Catastrophizing       0.50      0.33      0.40         6\n",
      "      Control Fallacies       0.64      0.54      0.58        13\n",
      "    Emotional Reasoning       1.00      0.64      0.78        11\n",
      "      Fallacy Of Change       0.33      0.50      0.40         2\n",
      "    Fallacy Of Fairness       0.43      0.50      0.46         6\n",
      "              Filtering       0.54      0.70      0.61        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.67      0.67      0.67         6\n",
      " Jumping to Conclusions       0.60      0.33      0.43         9\n",
      "                     No       0.00      0.00      0.00        11\n",
      "     Overgeneralization       0.44      0.50      0.47         8\n",
      "        Personalization       0.33      1.00      0.50         5\n",
      "              Polarized       0.22      0.33      0.27         6\n",
      "                Shoulds       1.00      0.43      0.60         7\n",
      "\n",
      "               accuracy                           0.53       119\n",
      "              macro avg       0.52      0.56      0.50       119\n",
      "           weighted avg       0.55      0.53      0.51       119\n",
      "\n",
      "0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "297e491db09c8e1cad3dcb7da858ce4188d439da100fd9118e665979685ce609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
