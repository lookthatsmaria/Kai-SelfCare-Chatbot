{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading Dataset \n",
    "df = pd.read_csv('Dataset_Cognitive_Distortions.tsv', sep='\\t', header=0)\n",
    "#df.shape\n",
    "\n",
    "# Divide data between training and test data\n",
    "X = df['Phrase']\n",
    "y = df['Cognitive Distortion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .8)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTINOMIAL NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 54 candidates, totalling 540 fits\n",
      "Train Accuracy : 0.994\n",
      "Test Accuracy : 0.689\n",
      "Best Accuracy Through Grid Search : 0.683\n",
      "Best Parameters :  {'mnb__alpha': 0.1, 'smote__k_neighbors': 10}\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  5  0  1  0  0  0  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  5  0  0  0  0  1  0  0  0  0  0  0  0  0]\n",
      " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  3  1  0  1  0  1  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  6  0  1  0  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  1  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  2  0  0  0  2  0  0  4  0  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  1  4  1  1  2  0]\n",
      " [ 0  1  0  1  0  1  0  0  0  0  0  0  5  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  1  0  0  0  1  0  3  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      1.00      1.00         3\n",
      "                Blaming       0.62      0.71      0.67         7\n",
      "        Catastrophizing       0.83      0.83      0.83         6\n",
      "      Control Fallacies       0.64      0.54      0.58        13\n",
      "    Emotional Reasoning       1.00      1.00      1.00        11\n",
      "      Fallacy Of Change       0.67      1.00      0.80         2\n",
      "    Fallacy Of Fairness       1.00      0.50      0.67         6\n",
      "              Filtering       0.50      0.60      0.55        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.56      0.83      0.67         6\n",
      " Jumping to Conclusions       0.80      0.44      0.57         9\n",
      "                     No       0.44      0.36      0.40        11\n",
      "     Overgeneralization       0.71      0.62      0.67         8\n",
      "        Personalization       0.71      1.00      0.83         5\n",
      "              Polarized       0.33      0.50      0.40         6\n",
      "                Shoulds       1.00      0.71      0.83         7\n",
      "\n",
      "               accuracy                           0.69       119\n",
      "              macro avg       0.73      0.73      0.71       119\n",
      "           weighted avg       0.71      0.69      0.69       119\n",
      "\n",
      "0.6890756302521008\n"
     ]
    }
   ],
   "source": [
    "textclassifier = Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('mnb', MultinomialNB())\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ],\n",
    "          'mnb__alpha': [0.01, 0.1, 0.3, 0.5, 1.0, 10, ]}\n",
    "          \n",
    "multinomial_nb_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_nb_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_nb_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_nb_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_nb_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MULTINOMIAL LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 720 candidates, totalling 7200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "2700 fits failed out of a total of 7200.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "450 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.07562057 0.0777039  0.0777039\n",
      " 0.0777039  0.0777039  0.0777039  0.0777039  0.0777039  0.0777039\n",
      " 0.56724291 0.55882092 0.56733156 0.57353723 0.57384752 0.58191489\n",
      " 0.58009752 0.57367021 0.59060284 0.56724291 0.55882092 0.56733156\n",
      " 0.57353723 0.57384752 0.58191489 0.58009752 0.57367021 0.59060284\n",
      " 0.56099291 0.55673759 0.56737589 0.57140957 0.57176418 0.58191489\n",
      " 0.58847518 0.56733156 0.59268617 0.52544326 0.51901596 0.51910461\n",
      " 0.53169326 0.51919326 0.52109929 0.52969858 0.52522163 0.5402039\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.07367021 0.06941489 0.07575355\n",
      " 0.07579787 0.08195922 0.07575355 0.07371454 0.08413121 0.06941489\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.05465426 0.0545656  0.06103723\n",
      " 0.05678191 0.0545656  0.05886525 0.0483156  0.05682624 0.05669326\n",
      " 0.57570922 0.56932624 0.57783688 0.58404255 0.58435284 0.59033688\n",
      " 0.58847518 0.58413121 0.60115248 0.57570922 0.56932624 0.57783688\n",
      " 0.58404255 0.58435284 0.59033688 0.58847518 0.58413121 0.60115248\n",
      " 0.57570922 0.56932624 0.57783688 0.58195922 0.5822695  0.59033688\n",
      " 0.58847518 0.58413121 0.60115248 0.57575355 0.56307624 0.57570922\n",
      " 0.57566489 0.57805851 0.58191489 0.57792553 0.58204787 0.5947695\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.09862589 0.10088652 0.09237589\n",
      " 0.1008422  0.09875887 0.1008422  0.09875887 0.09875887 0.1008422\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.16369681 0.15527482 0.16374113\n",
      " 0.16578014 0.16369681 0.1554078  0.13860816 0.14911348 0.16573582\n",
      " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
      " 0.58838652 0.5945922  0.61578014 0.59680851 0.59024823 0.58408688\n",
      " 0.58617021 0.59689716 0.60079787 0.58838652 0.5945922  0.61578014\n",
      " 0.59680851 0.59024823 0.58408688 0.58617021 0.59689716 0.60079787\n",
      " 0.58838652 0.5945922  0.61578014 0.59468085 0.58603723 0.58200355\n",
      " 0.58617021 0.59902482 0.60496454 0.59671986 0.59667553 0.6179078\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.3106383  0.32544326 0.27096631\n",
      " 0.29401596 0.31697695 0.3043883  0.2918883  0.29809397 0.30434397\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.32101064 0.32960993 0.28125\n",
      " 0.30873227 0.28351064 0.29175532 0.28156028 0.29175532 0.28320035\n",
      " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
      " 0.60305851 0.60305851 0.63049645 0.60296986 0.60279255 0.60718085\n",
      " 0.60088652 0.60939716 0.60921986 0.60305851 0.60305851 0.63049645\n",
      " 0.60296986 0.60279255 0.60718085 0.60088652 0.60939716 0.60921986\n",
      " 0.60305851 0.60305851 0.63049645 0.60301418 0.59862589 0.60509752\n",
      " 0.60500887 0.6114805  0.61130319 0.6072695  0.60509752 0.63049645\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.41143617 0.40935284 0.38661348\n",
      " 0.39268617 0.39507979 0.40323582 0.38444149 0.38639184 0.39073582\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.45602837 0.4623227  0.44135638\n",
      " 0.43275709 0.43289007 0.43927305 0.44140071 0.45164007 0.42646277\n",
      " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
      " 0.64069149 0.65124113 0.66609043 0.64490248 0.64277482 0.64924645\n",
      " 0.64281915 0.63674645 0.63005319 0.64069149 0.65124113 0.66609043\n",
      " 0.64490248 0.64277482 0.64924645 0.64281915 0.63674645 0.63005319\n",
      " 0.64069149 0.65124113 0.66609043 0.64703014 0.64069149 0.64924645\n",
      " 0.64281915 0.63253546 0.62796986 0.63860816 0.64911348 0.66187943\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan 0.51050532 0.50629433 0.49804965\n",
      " 0.48949468 0.49818262 0.50629433 0.50234929 0.4981383  0.50647163\n",
      " 0.64689716 0.65350177 0.67420213 0.67460106 0.64893617 0.66804078\n",
      " 0.67437943 0.67637411 0.67659574 0.68071809 0.67429078 0.68280142\n",
      " 0.67863475 0.68488475 0.68067376 0.68701241 0.68692376 0.68280142\n",
      " 0.67242908 0.66187943 0.67655142 0.67242908 0.67659574 0.67030142\n",
      " 0.6787234  0.68289007 0.67863475 0.68705674 0.67442376 0.68696809\n",
      " 0.68071809 0.68705674 0.68492908 0.68289007 0.68284574 0.68488475]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1165: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=none)\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1181: UserWarning: Setting penalty=None will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.647\n",
      "Best Accuracy Through Grid Search : 0.687\n",
      "Best Parameters :  {'mlg__C': 0.01, 'mlg__penalty': 'none', 'mlg__solver': 'saga', 'smote__k_neighbors': 2}\n",
      "[[ 3  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  3  0  1  0  0  2  0  0  0  0  0  0  1  0  0]\n",
      " [ 0  0  5  0  0  0  0  0  0  1  0  0  0  0  0  0]\n",
      " [ 0  2  1  7  0  0  0  0  0  1  0  2  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  2  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  2  0  1  1  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  4  0  1  0  4  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  1  1  0  0  0  0  0  5  0  1  0  1  0]\n",
      " [ 0  0  0  0  0  0  1  1  0  1  1  4  1  1  1  0]\n",
      " [ 1  0  0  0  0  0  0  0  0  0  1  0  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  5  0  0]\n",
      " [ 0  0  1  1  0  0  0  1  0  0  0  0  1  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       0.75      1.00      0.86         3\n",
      "                Blaming       0.60      0.43      0.50         7\n",
      "        Catastrophizing       0.71      0.83      0.77         6\n",
      "      Control Fallacies       0.70      0.54      0.61        13\n",
      "    Emotional Reasoning       0.92      1.00      0.96        11\n",
      "      Fallacy Of Change       1.00      1.00      1.00         2\n",
      "    Fallacy Of Fairness       0.25      0.17      0.20         6\n",
      "              Filtering       0.50      0.40      0.44        10\n",
      "       Global Labelling       1.00      1.00      1.00         9\n",
      "Heaven's Reward Fallacy       0.45      0.83      0.59         6\n",
      " Jumping to Conclusions       0.71      0.56      0.63         9\n",
      "                     No       0.36      0.36      0.36        11\n",
      "     Overgeneralization       0.60      0.75      0.67         8\n",
      "        Personalization       0.71      1.00      0.83         5\n",
      "              Polarized       0.29      0.33      0.31         6\n",
      "                Shoulds       1.00      0.71      0.83         7\n",
      "\n",
      "               accuracy                           0.65       119\n",
      "              macro avg       0.66      0.68      0.66       119\n",
      "           weighted avg       0.65      0.65      0.64       119\n",
      "\n",
      "0.6470588235294118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('mlg', LogisticRegression(multi_class='multinomial', random_state=0, warm_start = True,  l1_ratio = 0.5))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ],\n",
    "          'mlg__penalty': ['l1', 'l2', 'elasticnet', 'none' ],\n",
    "          'mlg__C': [0.01, 0.1, 0.3, 0.5, 1.0,],\n",
    "          'mlg__solver': ['lbfgs', 'newton-cg'\n",
    "                          , 'sag', 'saga'],\n",
    "          }\n",
    "          \n",
    "multinomial_lg_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "multinomial_lg_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%multinomial_lg_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%multinomial_lg_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%multinomial_lg_grid.best_score_)\n",
    "print('Best Parameters : ',multinomial_lg_grid.best_params_)\n",
    "\n",
    "y_pred = multinomial_lg_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 90 candidates, totalling 900 fits\n",
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.672\n",
      "Best Accuracy Through Grid Search : 0.637\n",
      "Best Parameters :  {'smote__k_neighbors': 10, 'svm__C': 1.0, 'svm__decision_function_shape': 'ovo'}\n",
      "[[ 2  0  0  0  0  0  0  0  0  0  0  1  0  0  0  0]\n",
      " [ 0  3  0  1  0  0  0  0  0  0  0  2  0  1  0  0]\n",
      " [ 0  0  4  0  0  0  0  0  0  0  0  1  1  0  0  0]\n",
      " [ 0  0  0  7  0  0  0  0  0  0  0  6  0  0  0  0]\n",
      " [ 0  0  0  0 11  0  0  0  0  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  1  0  0  0  0  1  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  1  0  2  1  0  0  1]\n",
      " [ 0  0  0  0  0  0  0  4  0  0  0  5  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  9  0  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  1  0  5  0  0  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  6  2  0  0  1  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0 11  0  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  2  6  0  0  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  1  0  4  0  0]\n",
      " [ 0  0  0  1  0  0  0  0  0  0  0  3  0  0  2  0]\n",
      " [ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  2  5]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      0.67      0.80         3\n",
      "                Blaming       1.00      0.43      0.60         7\n",
      "        Catastrophizing       1.00      0.67      0.80         6\n",
      "      Control Fallacies       0.78      0.54      0.64        13\n",
      "    Emotional Reasoning       1.00      1.00      1.00        11\n",
      "      Fallacy Of Change       1.00      0.50      0.67         2\n",
      "    Fallacy Of Fairness       0.00      0.00      0.00         6\n",
      "              Filtering       0.67      0.40      0.50        10\n",
      "       Global Labelling       1.00      1.00      1.00         9\n",
      "Heaven's Reward Fallacy       0.83      0.83      0.83         6\n",
      " Jumping to Conclusions       0.86      0.67      0.75         9\n",
      "                     No       0.31      1.00      0.47        11\n",
      "     Overgeneralization       0.75      0.75      0.75         8\n",
      "        Personalization       0.80      0.80      0.80         5\n",
      "              Polarized       0.33      0.33      0.33         6\n",
      "                Shoulds       0.83      0.71      0.77         7\n",
      "\n",
      "               accuracy                           0.67       119\n",
      "              macro avg       0.76      0.64      0.67       119\n",
      "           weighted avg       0.75      0.67      0.67       119\n",
      "\n",
      "0.6722689075630253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('svm', svm.SVC(probability = True))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10,],\n",
    "        'svm__decision_function_shape': ['ovo', 'ovr'], \n",
    "        'svm__C': [0.01, 0.1, 0.3, 0.5, 1.0,],\n",
    "}\n",
    "\n",
    "svm_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=60, cv=10, verbose=5)\n",
    "svm_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%svm_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%svm_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%svm_grid.best_score_)\n",
    "print('Best Parameters : ',svm_grid.best_params_)\n",
    "\n",
    "y_pred = svm_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RANDOM FORESTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n",
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.689\n",
      "Best Accuracy Through Grid Search : 0.668\n",
      "Best Parameters :  {'smote__k_neighbors': 3}\n",
      "[[2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 1 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 4 0 0 0 0 0 1 0 0 0 0 0 1 0]\n",
      " [0 2 0 6 1 0 0 0 0 0 0 2 0 0 0 2]\n",
      " [0 0 0 0 9 0 0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 1 0 1 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 5 0 0 0 3 1 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 2 0 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 7 0 0 0 1 1]\n",
      " [0 0 1 0 0 0 0 1 0 0 0 9 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 5 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 1 0 0 0 0 1 0 0 1 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 7]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       1.00      0.67      0.80         3\n",
      "                Blaming       0.62      0.71      0.67         7\n",
      "        Catastrophizing       0.80      0.67      0.73         6\n",
      "      Control Fallacies       0.75      0.46      0.57        13\n",
      "    Emotional Reasoning       0.82      0.82      0.82        11\n",
      "      Fallacy Of Change       1.00      1.00      1.00         2\n",
      "    Fallacy Of Fairness       0.50      0.17      0.25         6\n",
      "              Filtering       0.56      0.50      0.53        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.75      0.50      0.60         6\n",
      " Jumping to Conclusions       1.00      0.78      0.88         9\n",
      "                     No       0.53      0.82      0.64        11\n",
      "     Overgeneralization       0.56      0.62      0.59         8\n",
      "        Personalization       0.83      1.00      0.91         5\n",
      "              Polarized       0.60      0.50      0.55         6\n",
      "                Shoulds       0.54      1.00      0.70         7\n",
      "\n",
      "               accuracy                           0.69       119\n",
      "              macro avg       0.73      0.70      0.70       119\n",
      "           weighted avg       0.71      0.69      0.68       119\n",
      "\n",
      "0.6890756302521008\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('rf', RandomForestClassifier(random_state=0))\n",
    "])\n",
    "\n",
    "params = {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ]}\n",
    "          \n",
    "\n",
    "rd_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "rd_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%rd_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%rd_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%rd_grid.best_score_)\n",
    "print('Best Parameters : ',rd_grid.best_params_)\n",
    "\n",
    "y_pred = rd_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 324 candidates, totalling 3240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "1080 fits failed out of a total of 3240.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1080 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\", line 215, in fit\n",
      "    return self._fit(X, y)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\neighbors\\_base.py\", line 562, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Metric 'minkowski' not valid for sparse input. Use sorted(sklearn.neighbors.VALID_METRICS_SPARSE['brute']) to get valid options. Metric can also be a callable function.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.07344858 0.07553191 0.06928191 0.06923759 0.06928191 0.07136525\n",
      " 0.06928191 0.07344858 0.06923759 0.12588652 0.12805851 0.1133422\n",
      " 0.12597518 0.11546986 0.11546986 0.11546986 0.12176418 0.11338652\n",
      " 0.40110816 0.41999113 0.3760195  0.39268617 0.4097961  0.41812943\n",
      " 0.42012411 0.37996454 0.42220745 0.51453901 0.52083333 0.49184397\n",
      " 0.50199468 0.50212766 0.52318262 0.52291667 0.52105496 0.53998227\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.08382092 0.09658688 0.07553191 0.09237589 0.08590426 0.08386525\n",
      " 0.07761525 0.08413121 0.07353723 0.1258422  0.13018617 0.10913121\n",
      " 0.13652482 0.11746454 0.12371454 0.10917553 0.12194149 0.12180851\n",
      " 0.38018617 0.39060284 0.36546986 0.36338652 0.37411348 0.40128546\n",
      " 0.39499113 0.35469858 0.40128546 0.50008865 0.51462766 0.48554965\n",
      " 0.4893617  0.48967199 0.48958333 0.49973404 0.49361702 0.50846631\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.0712766  0.09445922 0.06507092 0.10070922 0.07960993 0.08173759\n",
      " 0.0712766  0.08005319 0.08413121 0.12375887 0.13435284 0.12167553\n",
      " 0.14277482 0.11750887 0.1258422  0.11746454 0.12828014 0.1239805\n",
      " 0.36759752 0.37597518 0.35509752 0.35288121 0.34255319 0.39490248\n",
      " 0.38661348 0.34437057 0.38244681 0.48537234 0.49991135 0.47087766\n",
      " 0.4641844  0.46870567 0.48111702 0.48093972 0.47052305 0.48967199\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.08386525 0.09237589 0.09445922 0.10279255 0.08390957 0.08404255\n",
      " 0.07544326 0.08417553 0.07992021 0.12171986 0.12180851 0.12593085\n",
      " 0.14064716 0.11750887 0.13222518 0.10908688 0.11981383 0.1197695\n",
      " 0.36134752 0.34237589 0.33195922 0.33807624 0.3320922  0.38018617\n",
      " 0.37606383 0.32969858 0.35296986 0.47251773 0.4748227  0.45203901\n",
      " 0.44321809 0.46657801 0.47903369 0.47894504 0.43887411 0.46023936\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.0902039  0.09038121 0.1070922  0.10704787 0.07965426 0.09242021\n",
      " 0.07535461 0.0883422  0.08408688 0.13014184 0.12185284 0.1364805\n",
      " 0.13435284 0.11538121 0.12810284 0.11325355 0.12189716 0.11972518\n",
      " 0.34233156 0.33630319 0.33399823 0.33390957 0.33430851 0.37176418\n",
      " 0.35088652 0.32544326 0.35088652 0.47460106 0.47052305 0.44361702\n",
      " 0.41378546 0.43515071 0.47269504 0.45789007 0.43262411 0.44552305\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      " 0.09441489 0.09246454 0.10700355 0.10079787 0.08173759 0.09871454\n",
      " 0.0902039  0.09250887 0.08200355 0.12805851 0.1197695  0.13430851\n",
      " 0.13231383 0.11329787 0.13439716 0.11963652 0.11981383 0.12180851\n",
      " 0.33603723 0.33838652 0.31937057 0.31498227 0.31303191 0.36546986\n",
      " 0.34468085 0.30864362 0.32154255 0.45571809 0.45576241 0.44140071\n",
      " 0.40957447 0.42876773 0.46635638 0.44756206 0.43262411 0.43297872\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.529\n",
      "Best Accuracy Through Grid Search : 0.540\n",
      "Best Parameters :  {'knn__n_neighbors': 5, 'knn__p': 2, 'knn__weights': 'distance', 'smote__k_neighbors': 10}\n"
     ]
    }
   ],
   "source": [
    "textclassifier =Pipeline([\n",
    "  ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('knn', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "params = {'knn__n_neighbors': [5,6,7,8,9,10, ],\n",
    "          'knn__weights': ['uniform', 'distance'],\n",
    "          'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ],\n",
    "          'knn__p': [1, 2, 3],\n",
    "          }\n",
    "          \n",
    "knn_grid = GridSearchCV(estimator=textclassifier, param_grid=params, n_jobs=10, cv=10, verbose=5)\n",
    "knn_grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "print('Train Accuracy : %.3f'%knn_grid.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%knn_grid.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%knn_grid.best_score_)\n",
    "print('Best Parameters : ',knn_grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 3 0 1 0 0 2 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 2 2 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      " [1 3 0 7 0 0 1 0 0 0 0 1 0 0 0 0]\n",
      " [2 0 0 0 7 0 0 0 0 0 0 0 2 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0]\n",
      " [1 0 0 0 0 0 3 0 0 0 0 0 1 1 0 0]\n",
      " [0 0 1 0 0 0 0 7 0 1 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 4 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 2 0 0 3 0 0 1 3 0]\n",
      " [1 0 0 0 0 1 1 3 0 1 0 0 0 2 2 0]\n",
      " [1 0 1 0 0 0 0 0 0 0 1 0 4 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0]\n",
      " [0 0 0 1 0 1 0 0 1 0 0 0 0 1 2 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 1 0 2 3]]\n",
      "                         precision    recall  f1-score   support\n",
      "\n",
      "     Always Being Right       0.33      1.00      0.50         3\n",
      "                Blaming       0.50      0.43      0.46         7\n",
      "        Catastrophizing       0.50      0.33      0.40         6\n",
      "      Control Fallacies       0.64      0.54      0.58        13\n",
      "    Emotional Reasoning       1.00      0.64      0.78        11\n",
      "      Fallacy Of Change       0.33      0.50      0.40         2\n",
      "    Fallacy Of Fairness       0.43      0.50      0.46         6\n",
      "              Filtering       0.54      0.70      0.61        10\n",
      "       Global Labelling       0.82      1.00      0.90         9\n",
      "Heaven's Reward Fallacy       0.67      0.67      0.67         6\n",
      " Jumping to Conclusions       0.60      0.33      0.43         9\n",
      "                     No       0.00      0.00      0.00        11\n",
      "     Overgeneralization       0.44      0.50      0.47         8\n",
      "        Personalization       0.33      1.00      0.50         5\n",
      "              Polarized       0.22      0.33      0.27         6\n",
      "                Shoulds       1.00      0.43      0.60         7\n",
      "\n",
      "               accuracy                           0.53       119\n",
      "              macro avg       0.52      0.56      0.50       119\n",
      "           weighted avg       0.55      0.53      0.51       119\n",
      "\n",
      "0.5294117647058824\n"
     ]
    }
   ],
   "source": [
    "y_pred = multinomial_nb_grid.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "297e491db09c8e1cad3dcb7da858ce4188d439da100fd9118e665979685ce609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
