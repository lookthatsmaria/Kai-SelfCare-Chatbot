{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REQUIREMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\usuario\\desktop\\kai-selfcare-chatbot\\.venv\\lib\\site-packages (1.7.2)\n",
      "Requirement already satisfied: scipy in c:\\users\\usuario\\desktop\\kai-selfcare-chatbot\\.venv\\lib\\site-packages (from xgboost) (1.9.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\usuario\\desktop\\kai-selfcare-chatbot\\.venv\\lib\\site-packages (from xgboost) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost\n",
    "pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Importing Libraries\n",
    "\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import tensorflow as tf \n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"wordnet\")\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from imblearn.pipeline import Pipeline, make_pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import svm \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Loading The Dataset \n",
    "df = pd.read_csv('Dataset_Cognitive_Distortions.tsv', sep='\\t', header=0)\n",
    "#df = df.drop(df.columns[[2, 3, 4]], axis=1)  # df.columns is zero-based pd.Index\n",
    "df.shape\n",
    "\n",
    "#Data Preprocessing \n",
    "df['Phrase'] = df['Phrase'].str.lower()\n",
    "\n",
    "\n",
    "# Divide data between training and test data\n",
    "X = df['Phrase']\n",
    "y = df['Cognitive Distortion']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0, train_size = .8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 43 candidates, totalling 430 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "30 fits failed out of a total of 430.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\imblearn\\pipeline.py\", line 297, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\xgboost\\core.py\", line 620, in inner_f\n",
      "    return func(**kwargs)\n",
      "  File \"c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\xgboost\\sklearn.py\", line 1466, in fit\n",
      "    raise ValueError(\n",
      "ValueError: Invalid classes inferred from unique values of `y`.  Expected: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15], got ['Always Being Right' 'Blaming' 'Catastrophizing' 'Control Fallacies'\n",
      " 'Emotional Reasoning' 'Fallacy Of Change' 'Fallacy Of Fairness'\n",
      " 'Filtering' 'Global Labelling' \"Heaven's Reward Fallacy\"\n",
      " 'Jumping to Conclusions' 'No' 'Overgeneralization' 'Personalization'\n",
      " 'Polarized' 'Shoulds']\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.6662234  0.6662234  0.66830674 0.66830674 0.65984043 0.65984043\n",
      " 0.6366578  0.6366578  0.62203014 0.62203014 0.56511525 0.56511525\n",
      " 0.61555851 0.60935284 0.62207447 0.62203014 0.60735816 0.62610816\n",
      " 0.61143617 0.6135195  0.63461879        nan 0.64281915        nan\n",
      " 0.67460106 0.62615248 0.62615248        nan 0.10500887 0.65554078\n",
      " 0.5839539  0.39268617 0.50199468 0.36338652 0.4893617  0.35288121\n",
      " 0.4641844  0.33807624 0.44321809 0.33390957 0.41378546 0.31498227\n",
      " 0.40957447]\n",
      "  warnings.warn(\n",
      "c:\\Users\\Usuario\\Desktop\\Kai-SelfCare-Chatbot\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1173: FutureWarning: `penalty='none'`has been deprecated in 1.2 and will be removed in 1.4. To keep the past behaviour, set `penalty=None`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy : 0.998\n",
      "Test Accuracy : 0.622\n",
      "Best Accuracy Through Grid Search : 0.675\n",
      "Best Parameters :  {'classifier': LogisticRegression(multi_class='multinomial', penalty='none', random_state=0), 'classifier__penalty': 'none'}\n"
     ]
    }
   ],
   "source": [
    "#Best model \n",
    "topiclassification = Pipeline([\n",
    "   ('vect', CountVectorizer()),\n",
    "   ('tfidf', TfidfTransformer()),\n",
    "   ('smote', SMOTE(random_state=0)),\n",
    "   ('classifier', MultinomialNB())\n",
    "])\n",
    "\n",
    "search_space = [{'classifier': [MultinomialNB()],\n",
    "                               'classifier__alpha': [0.01, 0.1, 0.3, 0.5, 1.0, 10, ],\n",
    "                               'classifier__fit_prior': [True, False]\n",
    "                 },\n",
    "                {'smote__k_neighbors': [2,3,4,5,6,7,8,9,10, ]}, \n",
    "                \n",
    "                {'classifier': [LogisticRegression(multi_class='multinomial', random_state=0)],\n",
    "                               'classifier__penalty': ['l1', 'l2', 'elasticnet', 'none']                \n",
    "                 },\n",
    "                {'classifier': [svm.SVC(probability = True)],\n",
    "                               'classifier__decision_function_shape': ['ovo', 'ovr']\n",
    "                },\n",
    "                {'classifier':  [XGBClassifier(objective='multi:softmax')]\n",
    "                },\n",
    "                {'classifier':  [AdaBoostClassifier(n_estimators=5000)]\n",
    "                },\n",
    "                {'classifier':  [RandomForestClassifier(random_state=0)]\n",
    "                },\n",
    "                {'classifier':  [LGBMClassifier(random_state=0)]\n",
    "                },\n",
    "                { 'classifier': [KNeighborsClassifier()],\n",
    "                                'classifier__n_neighbors': [5,6,7,8,9,10, ],\n",
    "                                'classifier__weights': ['uniform', 'distance']\n",
    "                },\n",
    "                ]\n",
    "\n",
    "best_model = GridSearchCV(estimator=topiclassification, param_grid=search_space, n_jobs=50, cv=10, verbose=5)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "print('Train Accuracy : %.3f'%best_model.best_estimator_.score(X_train, y_train))\n",
    "print('Test Accuracy : %.3f'%best_model.best_estimator_.score(X_test, y_test))\n",
    "print('Best Accuracy Through Grid Search : %.3f'%best_model.best_score_)\n",
    "print('Best Parameters : ',best_model.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "297e491db09c8e1cad3dcb7da858ce4188d439da100fd9118e665979685ce609"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
